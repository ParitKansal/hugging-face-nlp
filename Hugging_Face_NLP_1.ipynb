{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1j8PMMwL3B7Noa7ZvljW9GbuJALClrktG",
      "authorship_tag": "ABX9TyMHhhX7NJfgYDKPvDNtV68e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "27af2c27e090497891f47b81b06eb54a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_460b6f914ca74041abe795587b78fa0e",
              "IPY_MODEL_2cacc9af72c84359a248eb088540588f",
              "IPY_MODEL_47388de3a87f4b3d8b2b820d29b28e41"
            ],
            "layout": "IPY_MODEL_74e7e13b18d74b0691d1b2c0cf6d7bb2"
          }
        },
        "460b6f914ca74041abe795587b78fa0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a81b6a9a3e434bbcbe3013e3bcaf9001",
            "placeholder": "​",
            "style": "IPY_MODEL_2bf235dab7c4428d82de486321ddcd54",
            "value": "Map: 100%"
          }
        },
        "2cacc9af72c84359a248eb088540588f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ec80b53227b49c9803b16fdf317848f",
            "max": 408,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cbbef8647f624fc9b5e8d29d6819ace1",
            "value": 408
          }
        },
        "47388de3a87f4b3d8b2b820d29b28e41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0769c4be95254f17be2c703272b4467c",
            "placeholder": "​",
            "style": "IPY_MODEL_df537a6346b7467baa949b6893621b7c",
            "value": " 408/408 [00:00&lt;00:00, 1820.31 examples/s]"
          }
        },
        "74e7e13b18d74b0691d1b2c0cf6d7bb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a81b6a9a3e434bbcbe3013e3bcaf9001": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bf235dab7c4428d82de486321ddcd54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ec80b53227b49c9803b16fdf317848f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbbef8647f624fc9b5e8d29d6819ace1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0769c4be95254f17be2c703272b4467c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df537a6346b7467baa949b6893621b7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba8b6ba88d4e42fab3b715a3e54dd676": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_124ae8c5734847319627464b1a444335",
              "IPY_MODEL_d708e27dbe2741848eb3ea8472b85e26",
              "IPY_MODEL_6a3e23e64ecd43729767e2222fcc8cc3"
            ],
            "layout": "IPY_MODEL_70f9f03776804ec089db43d53c4827b7"
          }
        },
        "124ae8c5734847319627464b1a444335": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6d87b3128ba4affbe75fdf2747079b5",
            "placeholder": "​",
            "style": "IPY_MODEL_0974e3620ab74b6d98b7340178f29f78",
            "value": "100%"
          }
        },
        "d708e27dbe2741848eb3ea8472b85e26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f95a94cf7bba4ec88fcb61d6efd00d89",
            "max": 1377,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aa1f84026081415baf0ec1f4dcb83924",
            "value": 1376
          }
        },
        "6a3e23e64ecd43729767e2222fcc8cc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a78957af25d4479b95b7f15bf8f44a81",
            "placeholder": "​",
            "style": "IPY_MODEL_b5c89da80b06458fb24807c036392d49",
            "value": " 1376/1377 [02:49&lt;00:00,  8.33it/s]"
          }
        },
        "70f9f03776804ec089db43d53c4827b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6d87b3128ba4affbe75fdf2747079b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0974e3620ab74b6d98b7340178f29f78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f95a94cf7bba4ec88fcb61d6efd00d89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa1f84026081415baf0ec1f4dcb83924": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a78957af25d4479b95b7f15bf8f44a81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5c89da80b06458fb24807c036392d49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ParitKansal/hugging-face-nlp/blob/main/Hugging_Face_NLP_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeNm2quEcM8z",
        "outputId": "1d274905-4931-406c-8ab7-228330be98bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset  # Importing the `load_dataset` function from the Hugging Face Datasets library\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding  # Importing tools for tokenization and data collation\n",
        "\n",
        "# Load the GLUE MRPC dataset\n",
        "raw_datasets = load_dataset(\"glue\", \"mrpc\")  # The GLUE MRPC dataset contains sentence pairs for paraphrase detection.\n",
        "\n",
        "# Define the checkpoint for the tokenizer\n",
        "checkpoint = \"bert-base-uncased\"  # Use the BERT base model with uncased tokenization\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)  # Load the tokenizer associated with the specified checkpoint\n",
        "\n",
        "# Define a function to tokenize input examples\n",
        "def tokenize_function(example):\n",
        "    # Tokenize the sentence pairs with truncation to fit the model's input size\n",
        "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
        "\n",
        "# Apply the tokenize function to the entire dataset\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "# Use the map method with `batched=True` to tokenize all examples efficiently in batches\n",
        "\n",
        "# Create a data collator for dynamically padding input data\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "# This ensures input tensors are padded to the longest sequence in a batch, making them ready for model training"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "27af2c27e090497891f47b81b06eb54a",
            "460b6f914ca74041abe795587b78fa0e",
            "2cacc9af72c84359a248eb088540588f",
            "47388de3a87f4b3d8b2b820d29b28e41",
            "74e7e13b18d74b0691d1b2c0cf6d7bb2",
            "a81b6a9a3e434bbcbe3013e3bcaf9001",
            "2bf235dab7c4428d82de486321ddcd54",
            "1ec80b53227b49c9803b16fdf317848f",
            "cbbef8647f624fc9b5e8d29d6819ace1",
            "0769c4be95254f17be2c703272b4467c",
            "df537a6346b7467baa949b6893621b7c"
          ]
        },
        "id": "nGt4SNYqULLa",
        "outputId": "a2dc2744-f88c-4b16-e7f4-1c23d38a1dd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "27af2c27e090497891f47b81b06eb54a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove unnecessary columns from the tokenized dataset\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
        "# The columns \"sentence1\", \"sentence2\", and \"idx\" are no longer needed after tokenization,\n",
        "# so we remove them to keep only the relevant data for model training.\n",
        "\n",
        "# Rename the \"label\" column to \"labels\"\n",
        "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
        "# The model expects the column containing target values to be named \"labels\",\n",
        "# so we rename the \"label\" column accordingly.\n",
        "\n",
        "# Set the dataset format to PyTorch tensors\n",
        "tokenized_datasets.set_format(\"torch\")\n",
        "# This ensures the dataset outputs are in a format compatible with PyTorch during training.\n",
        "\n",
        "# Display the column names of the training dataset\n",
        "tokenized_datasets[\"train\"].column_names\n",
        "# This lists the final column names in the training dataset to verify the structure."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPUytiGIb_hv",
        "outputId": "4e7c9875-10dd-4773-a285-3f44d78f8d34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['labels', 'input_ids', 'token_type_ids', 'attention_mask']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader  # Import DataLoader to handle batching and shuffling of data\n",
        "\n",
        "# Create a DataLoader for the training dataset\n",
        "train_dataloader = DataLoader(\n",
        "    tokenized_datasets[\"train\"],  # Use the training split of the tokenized dataset\n",
        "    shuffle=True,  # Shuffle the data during training to improve model generalization\n",
        "    batch_size=8,  # Process data in batches of 8 samples\n",
        "    collate_fn=data_collator  # Use the data collator for dynamic padding of input sequences\n",
        ")\n",
        "\n",
        "# Create a DataLoader for the validation dataset\n",
        "eval_dataloader = DataLoader(\n",
        "    tokenized_datasets[\"validation\"],  # Use the validation split of the tokenized dataset\n",
        "    batch_size=8,  # Process data in batches of 8 samples\n",
        "    collate_fn=data_collator  # Use the same data collator for consistency\n",
        ")\n",
        "\n",
        "# The DataLoaders handle the tokenized datasets and ensure the data is prepared for the model\n",
        "# in the correct format with batching, padding, and shuffling (for training)."
      ],
      "metadata": {
        "id": "JFZwRoSNUPKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over the training DataLoader and retrieve the first batch\n",
        "for batch in train_dataloader:\n",
        "    break  # Break after retrieving the first batch\n",
        "\n",
        "# Display the shape of each element in the batch\n",
        "{k: v.shape for k, v in batch.items()}\n",
        "# Create a dictionary comprehension to display the shapes of all elements (tensors) in the batch.\n",
        "# Keys (`k`) represent the data types (e.g., 'input_ids', 'attention_mask', 'labels'),\n",
        "# and values (`v.shape`) represent the dimensions of the corresponding tensors.\n",
        "\n",
        "# This provides a quick summary of the batch structure, ensuring that the input format is as expected for model training."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0xPjhqRaCYf",
        "outputId": "045c9c4d-787b-4960-93e0-cd677846c4c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'labels': torch.Size([8]),\n",
              " 'input_ids': torch.Size([8, 69]),\n",
              " 'token_type_ids': torch.Size([8, 69]),\n",
              " 'attention_mask': torch.Size([8, 69])}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification  # Import the model class for sequence classification\n",
        "\n",
        "# Load a pre-trained BERT model for sequence classification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    checkpoint,  # Use the same checkpoint as the tokenizer (e.g., \"bert-base-uncased\")\n",
        "    num_labels=2  # Specify the number of labels for the classification task (binary classification in this case)\n",
        ")\n",
        "\n",
        "# The model is initialized with the pre-trained weights from the specified checkpoint,\n",
        "# and the classification head is adjusted to output predictions for two classes."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoipAHVrcxoW",
        "outputId": "1b4393af-0b23-4c3b-e97c-eb6b5e4d5f38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass the batch through the model to get outputs\n",
        "outputs = model(**batch)\n",
        "# The double-asterisk (`**`) unpacks the batch dictionary, passing its items as keyword arguments to the model.\n",
        "# Typical inputs include 'input_ids', 'attention_mask', and 'labels' for training.\n",
        "\n",
        "# Print the loss and logits from the model's outputs\n",
        "print(outputs.loss, outputs.logits.shape)\n",
        "# `outputs.loss` contains the computed loss for the batch (only if 'labels' are provided in the input).\n",
        "# `outputs.logits` contains the raw predictions (logits) from the model, which have the shape [batch_size, num_labels].\n",
        "# This helps verify that the model is processing the batch correctly and outputting the expected results."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chJ7n2g7dXDN",
        "outputId": "674bb23d-5a74-4bc8-afaf-0cabd2b2c33f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.7830, grad_fn=<NllLossBackward0>) torch.Size([8, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW  # Import the AdamW optimizer from the Hugging Face transformers library\n",
        "\n",
        "# Initialize the AdamW optimizer for training\n",
        "optimizer = AdamW(\n",
        "    model.parameters(),  # Pass the model's parameters to the optimizer, so it knows which parameters to update\n",
        "    lr=5e-5  # Set the learning rate to 5e-5 (a commonly used learning rate for fine-tuning transformers)\n",
        ")\n",
        "\n",
        "# AdamW (Adam with Weight Decay) is an adaptive learning rate optimization algorithm,\n",
        "# often used for training transformer-based models like BERT. The learning rate controls how much\n",
        "# to adjust the model's parameters with respect to the computed gradients during training."
      ],
      "metadata": {
        "id": "pIxP-q5mdqfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import get_scheduler  # Import the function to get a learning rate scheduler\n",
        "\n",
        "# Set the number of training epochs\n",
        "num_epochs = 3  # The number of times the model will iterate over the entire training dataset\n",
        "\n",
        "# Calculate the total number of training steps based on the number of epochs and the number of batches per epoch\n",
        "num_training_steps = num_epochs * len(train_dataloader)\n",
        "# Each epoch consists of several steps, one for each batch in the training dataloader.\n",
        "# Multiply the number of epochs by the number of batches to get the total number of steps.\n",
        "\n",
        "# Initialize the learning rate scheduler to adjust the learning rate during training\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",  # Use a linear learning rate scheduler, where the learning rate decreases linearly from the initial value\n",
        "    optimizer=optimizer,  # The optimizer to which the scheduler is applied\n",
        "    num_warmup_steps=0,  # No warm-up steps, meaning the learning rate starts at the initial value immediately\n",
        "    num_training_steps=num_training_steps,  # The total number of training steps\n",
        ")\n",
        "\n",
        "# Print the total number of training steps to verify the calculation\n",
        "print(num_training_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDN1sybad7Xe",
        "outputId": "c96c9152-e010-4cf3-c20b-a05a904e96f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1377\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch  # Import the PyTorch library\n",
        "\n",
        "# Check if a GPU (CUDA-enabled device) is available, otherwise use the CPU\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "# If CUDA is available, the model will use the GPU for faster training. Otherwise, it will default to the CPU.\n",
        "\n",
        "# Move the model to the selected device (GPU or CPU)\n",
        "model.to(device)\n",
        "# This ensures that the model's parameters and computations are performed on the chosen device (GPU or CPU).\n",
        "\n",
        "# Print the device (either 'cuda' or 'cpu') to verify which device is being used\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_8y_mLoe4D5",
        "outputId": "32f1b6ab-76ce-4a17-e047-e5924f3f6adc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm  # Import the tqdm library for progress bars in the console\n",
        "\n",
        "# Initialize the progress bar to track the number of training steps\n",
        "progress_bar = tqdm(range(num_training_steps))  # num_training_steps was calculated earlier\n",
        "\n",
        "# Set the model to training mode\n",
        "model.train()\n",
        "\n",
        "# Loop over the number of epochs\n",
        "for epoch in range(num_epochs):\n",
        "    # Iterate over the batches in the training DataLoader\n",
        "    for batch in train_dataloader:\n",
        "        # Move the batch to the correct device (GPU or CPU)\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        # Pass the batch through the model\n",
        "        outputs = model(**batch)\n",
        "        # Get the loss from the model's outputs\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Backpropagate the loss to compute gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the model parameters using the optimizer\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate using the scheduler\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        # Reset the gradients after the optimization step\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Update the progress bar by one step\n",
        "        progress_bar.update(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "ba8b6ba88d4e42fab3b715a3e54dd676",
            "124ae8c5734847319627464b1a444335",
            "d708e27dbe2741848eb3ea8472b85e26",
            "6a3e23e64ecd43729767e2222fcc8cc3",
            "70f9f03776804ec089db43d53c4827b7",
            "f6d87b3128ba4affbe75fdf2747079b5",
            "0974e3620ab74b6d98b7340178f29f78",
            "f95a94cf7bba4ec88fcb61d6efd00d89",
            "aa1f84026081415baf0ec1f4dcb83924",
            "a78957af25d4479b95b7f15bf8f44a81",
            "b5c89da80b06458fb24807c036392d49"
          ]
        },
        "id": "X3vyi_CTfAk3",
        "outputId": "18a5c5e1-c70b-4cd4-9306-016a00c73447"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1377 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba8b6ba88d4e42fab3b715a3e54dd676"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate  # Import the 'evaluate' library to handle evaluation metrics\n",
        "\n",
        "# Load the GLUE MRPC evaluation metric\n",
        "metric = evaluate.load(\"glue\", \"mrpc\")\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "# The model is switched to evaluation mode, which deactivates layers like dropout for inference.\n",
        "\n",
        "# Iterate over the batches in the evaluation DataLoader\n",
        "for batch in eval_dataloader:\n",
        "    # Move the batch to the correct device (GPU or CPU)\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "    # Disable gradient computation (no backpropagation during evaluation)\n",
        "    with torch.no_grad():\n",
        "        # Pass the batch through the model to get the outputs\n",
        "        outputs = model(**batch)\n",
        "\n",
        "    # Extract the logits (raw predictions before softmax)\n",
        "    logits = outputs.logits\n",
        "\n",
        "    # Get the predicted class by selecting the class with the highest logit value\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "    # Add the predictions and the true labels to the metric for evaluation\n",
        "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
        "\n",
        "# Compute and return the final evaluation metrics (e.g., accuracy, F1-score)\n",
        "metric.compute()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIFurXXdfW8s",
        "outputId": "1f307696-0ef2-4f5c-f611-4b55c534f0ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.8602941176470589, 'f1': 0.902229845626072}"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "save_path = \"/content/drive/My Drive/saved_model1\"\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pc018rAxocm1",
        "outputId": "08c90f49-7548-48bd-e3c5-483a522b4d75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/My Drive/saved_model1/tokenizer_config.json',\n",
              " '/content/drive/My Drive/saved_model1/special_tokens_map.json',\n",
              " '/content/drive/My Drive/saved_model1/vocab.txt',\n",
              " '/content/drive/My Drive/saved_model1/added_tokens.json',\n",
              " '/content/drive/My Drive/saved_model1/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model and tokenizer\n",
        "model.save_pretrained(\"./saved_model\")\n",
        "tokenizer.save_pretrained(\"./saved_model\")\n",
        "\n",
        "# Optionally, save the optimizer and scheduler if you want to resume training later\n",
        "torch.save(optimizer.state_dict(), \"./saved_model/optimizer.pt\")\n",
        "torch.save(lr_scheduler.state_dict(), \"./saved_model/lr_scheduler.pt\")"
      ],
      "metadata": {
        "id": "k6m7Eb71f3r5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "# Load the saved model and tokenizer\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"./saved_model\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./saved_model\")\n",
        "\n",
        "# If you want to load optimizer and scheduler for resuming training\n",
        "optimizer.load_state_dict(torch.load(\"./saved_model/optimizer.pt\"))\n",
        "lr_scheduler.load_state_dict(torch.load(\"./saved_model/lr_scheduler.pt\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wo5KFQrNg1jZ",
        "outputId": "b4d28dd4-0338-4225-967f-b2d168aab774"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-53-f456769acdae>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  optimizer.load_state_dict(torch.load(\"./saved_model/optimizer.pt\"))\n",
            "<ipython-input-53-f456769acdae>:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  lr_scheduler.load_state_dict(torch.load(\"./saved_model/lr_scheduler.pt\"))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prediction(text1, text2):\n",
        "    # Tokenize the input text using the tokenizer\n",
        "    inputs = tokenizer(text1, text2, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "    # Move both inputs and the model to the same device\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}  # Move inputs to the device\n",
        "    model.to(device)  # Ensure the model is on the same device as inputs\n",
        "\n",
        "    # Put the model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Perform inference without computing gradients\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Get the logits (raw predictions)\n",
        "    logits = outputs.logits\n",
        "\n",
        "    # Get the predicted class (0 or 1 for MRPC task)\n",
        "    prediction = torch.argmax(logits, dim=-1).item()\n",
        "\n",
        "    return prediction\n",
        "\n",
        "# Example usage\n",
        "text1 = \"The quick brown fox jumps over the lazy dog.\"\n",
        "text2 = \"A fast brown fox leaps over a lazy dog.\"\n",
        "prediction = make_prediction(text1, text2)\n",
        "\n",
        "print(f\"Prediction: {prediction}\")  # 0 or 1 based on the MRPC task (paraphrase or not)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZA5X_Knhg4t",
        "outputId": "d2bb565b-d4ce-4c77-f2c7-969f6c3a84a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: 1\n"
          ]
        }
      ]
    }
  ]
}