{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[],"dockerImageVersionId":30824,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T14:28:04.858907Z","iopub.execute_input":"2024-12-27T14:28:04.859231Z","iopub.status.idle":"2024-12-27T14:28:15.088125Z","shell.execute_reply.started":"2024-12-27T14:28:04.859208Z","shell.execute_reply":"2024-12-27T14:28:15.086652Z"}},"outputs":[{"name":"stdout","text":"Collecting datasets\n  Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting dill<0.3.9,>=0.3.0\n  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from datasets) (6.0.2)\nCollecting aiohttp\n  Downloading aiohttp-3.11.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting multiprocess<0.70.17\n  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/site-packages (from datasets) (18.1.0)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/site-packages (from datasets) (4.67.1)\nCollecting xxhash\n  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from datasets) (3.16.1)\nRequirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/site-packages (from datasets) (0.27.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/site-packages (from datasets) (2.2.3)\nCollecting fsspec[http]<=2024.9.0,>=2023.1.0\n  Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting async-timeout<6.0,>=4.0\n  Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (24.3.0)\nCollecting aiosignal>=1.1.2\n  Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\nCollecting aiohappyeyeballs>=2.3.0\n  Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\nCollecting frozenlist>=1.1.1\n  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.9/241.9 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting multidict<7.0,>=4.5\n  Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting propcache>=0.2.0\n  Downloading propcache-0.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (205 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.1/205.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting yarl<2.0,>=1.17.0\n  Downloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.0)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.2.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.12.14)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nInstalling collected packages: xxhash, propcache, multidict, fsspec, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2024.10.0\n    Uninstalling fsspec-2024.10.0:\n      Successfully uninstalled fsspec-2024.10.0\n  Attempting uninstall: dill\n    Found existing installation: dill 0.3.9\n    Uninstalling dill-0.3.9:\n      Successfully uninstalled dill-0.3.9\nSuccessfully installed aiohappyeyeballs-2.4.4 aiohttp-3.11.11 aiosignal-1.3.2 async-timeout-5.0.1 datasets-3.2.0 dill-0.3.8 frozenlist-1.5.0 fsspec-2024.9.0 multidict-6.1.0 multiprocess-0.70.16 propcache-0.2.1 xxhash-3.5.0 yarl-1.18.3\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from datasets import load_dataset  # Importing the `load_dataset` function from the Hugging Face Datasets library\nfrom transformers import AutoTokenizer, DataCollatorWithPadding  # Importing tools for tokenization and data collation","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-27T14:28:15.088869Z","iopub.execute_input":"2024-12-27T14:28:15.089112Z","iopub.status.idle":"2024-12-27T14:29:06.018156Z","shell.execute_reply.started":"2024-12-27T14:28:15.089088Z","shell.execute_reply":"2024-12-27T14:29:06.016122Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n/usr/local/lib/python3.10/site-packages/torch_xla/__init__.py:202: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n  warnings.warn(\nWARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1735309732.601995      10 common_lib.cc:798] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: === \nlearning/45eac/tfrc/runtime/common_lib.cc:479\nE1227 14:28:52.633075135     448 oauth2_credentials.cc:238]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {created_time:\"2024-12-27T14:28:52.6330418+00:00\", grpc_status:2}\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Load the GLUE MRPC dataset\nraw_datasets = load_dataset(\"glue\", \"mrpc\")  # The GLUE MRPC dataset contains sentence pairs for paraphrase detection.\nraw_datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T14:29:06.019087Z","iopub.execute_input":"2024-12-27T14:29:06.019661Z","iopub.status.idle":"2024-12-27T14:29:11.288663Z","shell.execute_reply.started":"2024-12-27T14:29:06.019631Z","shell.execute_reply":"2024-12-27T14:29:11.287533Z"}},"outputs":[{"name":"stderr","text":"Generating train split: 100%|██████████| 3668/3668 [00:00<00:00, 343999.89 examples/s]\nGenerating validation split: 100%|██████████| 408/408 [00:00<00:00, 136074.75 examples/s]\nGenerating test split: 100%|██████████| 1725/1725 [00:00<00:00, 354734.97 examples/s]\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx'],\n        num_rows: 3668\n    })\n    validation: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx'],\n        num_rows: 408\n    })\n    test: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx'],\n        num_rows: 1725\n    })\n})"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"raw_datasets['train']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T14:29:11.289631Z","iopub.execute_input":"2024-12-27T14:29:11.289892Z","iopub.status.idle":"2024-12-27T14:29:11.295463Z","shell.execute_reply.started":"2024-12-27T14:29:11.289866Z","shell.execute_reply":"2024-12-27T14:29:11.294509Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['sentence1', 'sentence2', 'label', 'idx'],\n    num_rows: 3668\n})"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"raw_datasets['train'][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T14:29:11.296703Z","iopub.execute_input":"2024-12-27T14:29:11.296934Z","iopub.status.idle":"2024-12-27T14:29:11.350429Z","shell.execute_reply.started":"2024-12-27T14:29:11.296905Z","shell.execute_reply":"2024-12-27T14:29:11.348944Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n 'label': 1,\n 'idx': 0}"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"raw_datasets['train'][0]['idx']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T14:29:11.351223Z","iopub.execute_input":"2024-12-27T14:29:11.351455Z","iopub.status.idle":"2024-12-27T14:29:11.361845Z","shell.execute_reply.started":"2024-12-27T14:29:11.351434Z","shell.execute_reply":"2024-12-27T14:29:11.360741Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# Define the checkpoint for the tokenizer\ncheckpoint = \"bert-base-uncased\"  # Use the BERT base model with uncased tokenization\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)  # Load the tokenizer associated with the specified checkpoint","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T14:29:11.362954Z","iopub.execute_input":"2024-12-27T14:29:11.363219Z","iopub.status.idle":"2024-12-27T14:29:13.023361Z","shell.execute_reply.started":"2024-12-27T14:29:11.363195Z","shell.execute_reply":"2024-12-27T14:29:13.021759Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Define a function to tokenize input examples\ndef tokenize_function(example):\n    # Tokenize the sentence pairs with truncation to fit the model's input size\n    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n\n# Apply the tokenize function to the entire dataset\ntokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n# Use the map method with `batched=True` to tokenize all examples efficiently in batches","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T14:29:13.024281Z","iopub.execute_input":"2024-12-27T14:29:13.024520Z","iopub.status.idle":"2024-12-27T14:29:13.458965Z","shell.execute_reply.started":"2024-12-27T14:29:13.024497Z","shell.execute_reply":"2024-12-27T14:29:13.457978Z"}},"outputs":[{"name":"stderr","text":"Map: 100%|██████████| 3668/3668 [00:00<00:00, 13844.02 examples/s]\nMap: 100%|██████████| 408/408 [00:00<00:00, 12836.53 examples/s]\nMap: 100%|██████████| 1725/1725 [00:00<00:00, 16279.96 examples/s]\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"tokenized_datasets['train']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T14:29:13.460023Z","iopub.execute_input":"2024-12-27T14:29:13.460315Z","iopub.status.idle":"2024-12-27T14:29:13.464859Z","shell.execute_reply.started":"2024-12-27T14:29:13.460281Z","shell.execute_reply":"2024-12-27T14:29:13.464116Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n    num_rows: 3668\n})"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# Create a data collator for dynamically padding input data\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n# This ensures input tensors are padded to the longest sequence in a batch, making them ready for model training","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T14:29:13.465716Z","iopub.execute_input":"2024-12-27T14:29:13.465914Z","iopub.status.idle":"2024-12-27T14:29:13.485343Z","shell.execute_reply.started":"2024-12-27T14:29:13.465894Z","shell.execute_reply":"2024-12-27T14:29:13.484665Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Remove unnecessary columns from the tokenized dataset\ntokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n# The columns \"sentence1\", \"sentence2\", and \"idx\" are no longer needed after tokenization,\n# so we remove them to keep only the relevant data for model training.\n\n# Rename the \"label\" column to \"labels\"\ntokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n# The model expects the column containing target values to be named \"labels\",\n# so we rename the \"label\" column accordingly.\n\n# Set the dataset format to PyTorch tensors\ntokenized_datasets.set_format(\"torch\")\n# This ensures the dataset outputs are in a format compatible with PyTorch during training.\n\ntokenized_datasets[\"train\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T14:29:13.486487Z","iopub.execute_input":"2024-12-27T14:29:13.486753Z","iopub.status.idle":"2024-12-27T14:29:13.505221Z","shell.execute_reply.started":"2024-12-27T14:29:13.486731Z","shell.execute_reply":"2024-12-27T14:29:13.504419Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n    num_rows: 3668\n})"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"tokenized_datasets[\"train\"][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T14:29:13.506013Z","iopub.execute_input":"2024-12-27T14:29:13.506318Z","iopub.status.idle":"2024-12-27T14:29:13.523365Z","shell.execute_reply.started":"2024-12-27T14:29:13.506297Z","shell.execute_reply":"2024-12-27T14:29:13.522491Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"{'labels': tensor(1),\n 'input_ids': tensor([  101,  2572,  3217,  5831,  5496,  2010,  2567,  1010,  3183,  2002,\n          2170,  1000,  1996,  7409,  1000,  1010,  1997,  9969,  4487, 23809,\n          3436,  2010,  3350,  1012,   102,  7727,  2000,  2032,  2004,  2069,\n          1000,  1996,  7409,  1000,  1010,  2572,  3217,  5831,  5496,  2010,\n          2567,  1997,  9969,  4487, 23809,  3436,  2010,  3350,  1012,   102]),\n 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1]),\n 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1])}"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"from torch.utils.data import DataLoader  # Import DataLoader to handle batching and shuffling of data\n\n# Create a DataLoader for the training dataset\ntrain_dataloader = DataLoader(\n    tokenized_datasets[\"train\"],  # Use the training split of the tokenized dataset\n    shuffle=True,  # Shuffle the data during training to improve model generalization\n    batch_size=16,  # Process data in batches of 8 samples\n    collate_fn=data_collator  # Use the data collator for dynamic padding of input sequences\n)\n\n# Create a DataLoader for the validation dataset\ntest_dataloader = DataLoader(\n    tokenized_datasets[\"validation\"],  # Use the validation split of the tokenized dataset\n    batch_size=16,  # Process data in batches of 8 samples\n    collate_fn=data_collator  # Use the same data collator for consistency\n)\n\n# The DataLoaders handle the tokenized datasets and ensure the data is prepared for the model\n# in the correct format with batching, padding, and shuffling (for training).","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T14:29:13.524343Z","iopub.execute_input":"2024-12-27T14:29:13.524578Z","iopub.status.idle":"2024-12-27T14:29:13.531237Z","shell.execute_reply.started":"2024-12-27T14:29:13.524556Z","shell.execute_reply":"2024-12-27T14:29:13.530317Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"for batch in train_dataloader:\n    print(batch['labels'].shape)\n    print(batch['input_ids'].shape)\n    print(batch['attention_mask'].shape)\n    print(batch['token_type_ids'].shape)\n    break  # Break after retrieving the first batch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T14:29:13.531738Z","iopub.execute_input":"2024-12-27T14:29:13.531921Z","iopub.status.idle":"2024-12-27T14:29:13.547586Z","shell.execute_reply.started":"2024-12-27T14:29:13.531902Z","shell.execute_reply":"2024-12-27T14:29:13.546715Z"}},"outputs":[{"name":"stdout","text":"torch.Size([16])\ntorch.Size([16, 65])\ntorch.Size([16, 65])\ntorch.Size([16, 65])\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification  # Import the model class for sequence classification\n\n# Load a pre-trained BERT model for sequence classification\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    checkpoint,  # Use the same checkpoint as the tokenizer (e.g., \"bert-base-uncased\")\n    num_labels=2  # Specify the number of labels for the classification task (binary classification in this case)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T14:29:13.548607Z","iopub.execute_input":"2024-12-27T14:29:13.548834Z","iopub.status.idle":"2024-12-27T14:29:17.339876Z","shell.execute_reply.started":"2024-12-27T14:29:13.548802Z","shell.execute_reply":"2024-12-27T14:29:17.338739Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Pass the batch through the model to get outputs\noutputs = model(**batch)\n# The double-asterisk (`**`) unpacks the batch dictionary, passing its items as keyword arguments to the model.\n# Typical inputs include 'input_ids', 'attention_mask', and 'labels' for training.\n# Print the loss and logits from the model's outputs\noutputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T14:29:17.341001Z","iopub.execute_input":"2024-12-27T14:29:17.341291Z","iopub.status.idle":"2024-12-27T14:29:17.678317Z","shell.execute_reply.started":"2024-12-27T14:29:17.341265Z","shell.execute_reply":"2024-12-27T14:29:17.677291Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"SequenceClassifierOutput(loss=tensor(0.6253, grad_fn=<NllLossBackward0>), logits=tensor([[0.0942, 0.5491],\n        [0.0561, 0.5852],\n        [0.0603, 0.5912],\n        [0.0642, 0.5767],\n        [0.0598, 0.5903],\n        [0.0680, 0.5779],\n        [0.0596, 0.5671],\n        [0.0794, 0.5937],\n        [0.0392, 0.6320],\n        [0.0623, 0.5751],\n        [0.0525, 0.5794],\n        [0.0660, 0.5965],\n        [0.0781, 0.5817],\n        [0.1130, 0.5657],\n        [0.0777, 0.6085],\n        [0.0866, 0.5870]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"from transformers import AdamW  # Import the AdamW optimizer from the Hugging Face transformers library\n\n# Initialize the AdamW optimizer for training\noptimizer = AdamW(\n    model.parameters(),  # Pass the model's parameters to the optimizer, so it knows which parameters to update\n    lr=5e-5  # Set the learning rate to 5e-5 (a commonly used learning rate for fine-tuning transformers)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T14:29:17.679313Z","iopub.execute_input":"2024-12-27T14:29:17.679630Z","iopub.status.idle":"2024-12-27T14:29:17.716342Z","shell.execute_reply.started":"2024-12-27T14:29:17.679600Z","shell.execute_reply":"2024-12-27T14:29:17.715140Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"len(train_dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T14:29:17.717630Z","iopub.execute_input":"2024-12-27T14:29:17.717864Z","iopub.status.idle":"2024-12-27T14:29:17.823996Z","shell.execute_reply.started":"2024-12-27T14:29:17.717841Z","shell.execute_reply":"2024-12-27T14:29:17.822446Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"230"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"from transformers import get_scheduler  # Import the function to get a learning rate scheduler\n\n# Set the number of training epochs\nnum_epochs = 3  # The number of times the model will iterate over the entire training dataset\n\n# Calculate the total number of training steps based on the number of epochs and the number of batches per epoch\nnum_training_steps = num_epochs * len(train_dataloader)\n\n# Initialize the learning rate scheduler to adjust the learning rate during training\nlr_scheduler = get_scheduler(\n    \"linear\",  # Use a linear learning rate scheduler, where the learning rate decreases linearly from the initial value\n    optimizer=optimizer,  # The optimizer to which the scheduler is applied\n    num_warmup_steps=0,  # No warm-up steps, meaning the learning rate starts at the initial value immediately\n    num_training_steps=num_training_steps,  # The total number of training steps\n)\n\n# Print the total number of training steps to verify the calculation\nprint(num_training_steps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T14:29:17.825090Z","iopub.execute_input":"2024-12-27T14:29:17.825477Z","iopub.status.idle":"2024-12-27T14:29:17.960129Z","shell.execute_reply.started":"2024-12-27T14:29:17.825450Z","shell.execute_reply":"2024-12-27T14:29:17.958995Z"}},"outputs":[{"name":"stdout","text":"690\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"import torch  \ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nmodel.to(device)\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T14:29:17.961511Z","iopub.execute_input":"2024-12-27T14:29:17.961797Z","iopub.status.idle":"2024-12-27T14:29:18.176873Z","shell.execute_reply.started":"2024-12-27T14:29:17.961773Z","shell.execute_reply":"2024-12-27T14:29:18.175752Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"device(type='cpu')"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"len(train_dataloader.dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T14:29:18.178157Z","iopub.execute_input":"2024-12-27T14:29:18.178396Z","iopub.status.idle":"2024-12-27T14:29:18.324954Z","shell.execute_reply.started":"2024-12-27T14:29:18.178375Z","shell.execute_reply":"2024-12-27T14:29:18.323588Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"3668"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score\n\ndef train(train_dataloader, model, optimizer, lr_scheduler, device):\n    model.train()\n    total_loss = 0\n    total_correct = 0  # Variable to track the number of correct predictions\n    total_samples = 0  # Variable to track the total number of samples\n    all_predictions = []\n    all_labels = []\n    \n    for i, batch in enumerate(train_dataloader):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        total_loss += loss.item()\n\n        # Collect predictions and labels\n        logits = outputs.logits\n        predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n        labels = batch[\"labels\"].cpu().numpy()\n        all_predictions.extend(predictions)\n        all_labels.extend(labels)\n\n        # Calculate accuracy for this batch\n        correct = (predictions == labels).sum()\n        total_correct += correct\n        total_samples += len(labels)\n\n        # Backpropagation and optimization\n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        if i%20 == 0:\n            print(f\"Batch {i+1}/{len(train_dataloader)}, Loss: {loss.item():.4f}\")\n\n    # Calculate overall metrics for training\n    avg_loss = total_loss / len(train_dataloader)\n    accuracy = total_correct / total_samples\n    precision = precision_score(all_labels, all_predictions, average=\"weighted\")\n    recall = recall_score(all_labels, all_predictions, average=\"weighted\")\n    f1 = f1_score(all_labels, all_predictions, average=\"weighted\")\n    print(f\"Training - Avg Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n\n\ndef test(test_dataloader, model, device):\n    model.eval()\n    test_loss = 0\n    total_correct = 0  # Variable to track the number of correct predictions\n    total_samples = 0  # Variable to track the total number of samples\n    all_predictions = []\n    all_labels = []\n    num_batches = len(test_dataloader)\n\n    with torch.no_grad():\n        for batch in test_dataloader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(**batch)\n            test_loss += outputs.loss.item()\n\n            # Collect predictions and labels\n            logits = outputs.logits\n            predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n            labels = batch[\"labels\"].cpu().numpy()\n            all_predictions.extend(predictions)\n            all_labels.extend(labels)\n\n            # Calculate accuracy for this batch\n            correct = (predictions == labels).sum()\n            total_correct += correct\n            total_samples += len(labels)\n\n    # Calculate overall metrics for testing\n    avg_loss = test_loss / num_batches\n    accuracy = total_correct / total_samples\n    precision = precision_score(all_labels, all_predictions, average=\"weighted\")\n    recall = recall_score(all_labels, all_predictions, average=\"weighted\")\n    f1 = f1_score(all_labels, all_predictions, average=\"weighted\")\n    print(f\"Testing - Avg Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n\n\n# Main training/testing loop\nepochs = 2\nfor t in range(epochs):\n    print(f\"Epoch {t+1}/{epochs}\")\n    train(train_dataloader, model, optimizer, lr_scheduler, device)\n    test(test_dataloader, model, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T14:29:36.552022Z","iopub.execute_input":"2024-12-27T14:29:36.552409Z","iopub.status.idle":"2024-12-27T14:33:38.699904Z","shell.execute_reply.started":"2024-12-27T14:29:36.552379Z","shell.execute_reply":"2024-12-27T14:33:38.698535Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/2\nBatch 1/230, Loss: 0.5984\nBatch 21/230, Loss: 0.5772\nBatch 41/230, Loss: 0.4018\nBatch 61/230, Loss: 0.6699\nBatch 81/230, Loss: 0.4037\nBatch 101/230, Loss: 0.4855\nBatch 121/230, Loss: 0.4708\nBatch 141/230, Loss: 0.4643\nBatch 161/230, Loss: 0.4732\nBatch 181/230, Loss: 0.5547\nBatch 201/230, Loss: 0.4719\nBatch 221/230, Loss: 0.6496\nTraining - Avg Loss: 0.5092, Accuracy: 0.7563, Precision: 0.7466, Recall: 0.7563, F1 Score: 0.7451\nTesting - Avg Loss: 0.4015, Accuracy: 0.8309, Precision: 0.8378, Recall: 0.8309, F1 Score: 0.8332\nEpoch 2/2\nBatch 1/230, Loss: 0.2039\nBatch 21/230, Loss: 0.1024\nBatch 41/230, Loss: 0.1668\nBatch 61/230, Loss: 0.1761\nBatch 81/230, Loss: 0.0955\nBatch 101/230, Loss: 0.5019\nBatch 121/230, Loss: 0.3485\nBatch 141/230, Loss: 0.3453\nBatch 161/230, Loss: 0.0677\nBatch 181/230, Loss: 0.1383\nBatch 201/230, Loss: 0.1538\nBatch 221/230, Loss: 0.1935\nTraining - Avg Loss: 0.2463, Accuracy: 0.9081, Precision: 0.9078, Recall: 0.9081, F1 Score: 0.9080\nTesting - Avg Loss: 0.3534, Accuracy: 0.8578, Precision: 0.8606, Recall: 0.8578, F1 Score: 0.8506\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"import os\n\nsave_path = \"/kaggle/working/model\"\nos.makedirs(save_path, exist_ok=True)\n\nmodel.save_pretrained(save_path)\ntokenizer.save_pretrained(save_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T14:33:38.700887Z","iopub.execute_input":"2024-12-27T14:33:38.701188Z","iopub.status.idle":"2024-12-27T14:33:39.665094Z","shell.execute_reply.started":"2024-12-27T14:33:38.701160Z","shell.execute_reply":"2024-12-27T14:33:39.664089Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/model/tokenizer_config.json',\n '/kaggle/working/model/special_tokens_map.json',\n '/kaggle/working/model/vocab.txt',\n '/kaggle/working/model/added_tokens.json',\n '/kaggle/working/model/tokenizer.json')"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AutoTokenizer\n\n# Load the saved model and tokenizer\nmodel = AutoModelForSequenceClassification.from_pretrained(\"/kaggle/working/model\")\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/working/model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T14:35:47.609575Z","iopub.execute_input":"2024-12-27T14:35:47.609915Z","iopub.status.idle":"2024-12-27T14:35:47.704880Z","shell.execute_reply.started":"2024-12-27T14:35:47.609892Z","shell.execute_reply":"2024-12-27T14:35:47.704151Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"def make_prediction(text1, text2):\n    inputs = tokenizer(text1, text2, truncation=True, padding=True, return_tensors=\"pt\")\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    model.to(device)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    logits = outputs.logits\n    prediction = torch.argmax(logits, dim=-1).item()\n    return prediction\n\n# Example usage Check  2 sentences are equal\ntext1 = \"The quick brown fox jumps over the lazy dog.\"\ntext2 = \"A fast brown fox leaps over a lazy dog.\"\nprediction = make_prediction(text1, text2)\n\nprint(f\"Prediction: {prediction}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T14:38:04.465118Z","iopub.execute_input":"2024-12-27T14:38:04.465480Z","iopub.status.idle":"2024-12-27T14:38:04.507354Z","shell.execute_reply.started":"2024-12-27T14:38:04.465455Z","shell.execute_reply":"2024-12-27T14:38:04.506081Z"}},"outputs":[{"name":"stdout","text":"Prediction: 1\n","output_type":"stream"}],"execution_count":28}]}