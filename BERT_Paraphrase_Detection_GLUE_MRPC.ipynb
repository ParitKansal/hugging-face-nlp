{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[],"dockerImageVersionId":30827,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T04:45:06.831067Z","iopub.execute_input":"2024-12-28T04:45:06.831429Z","iopub.status.idle":"2024-12-28T04:45:10.610902Z","shell.execute_reply.started":"2024-12-28T04:45:06.831403Z","shell.execute_reply":"2024-12-28T04:45:10.609196Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.10/site-packages (3.2.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from datasets) (3.16.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/site-packages (from datasets) (18.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from datasets) (24.2)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/site-packages (from datasets) (3.5.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/site-packages (from datasets) (3.11.11)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: fsspec[http]<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/site-packages (from datasets) (2024.9.0)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/site-packages (from datasets) (4.67.1)\nRequirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/site-packages (from datasets) (0.27.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/site-packages (from datasets) (2.2.3)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (24.3.0)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.12.14)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.2.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"execution_count":66},{"cell_type":"code","source":"from datasets import load_dataset  # Importing the `load_dataset` function from the Hugging Face Datasets library\nfrom transformers import AutoTokenizer, DataCollatorWithPadding  # Importing tools for tokenization and data collation","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-28T04:45:10.612132Z","iopub.execute_input":"2024-12-28T04:45:10.612390Z","iopub.status.idle":"2024-12-28T04:45:10.616625Z","shell.execute_reply.started":"2024-12-28T04:45:10.612365Z","shell.execute_reply":"2024-12-28T04:45:10.615844Z"}},"outputs":[],"execution_count":67},{"cell_type":"code","source":"# Load the GLUE MRPC dataset\nraw_datasets = load_dataset(\"glue\", \"mrpc\")  # The GLUE MRPC dataset contains sentence pairs for paraphrase detection.\nraw_datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T04:45:10.617416Z","iopub.execute_input":"2024-12-28T04:45:10.617619Z","iopub.status.idle":"2024-12-28T04:45:12.810901Z","shell.execute_reply.started":"2024-12-28T04:45:10.617599Z","shell.execute_reply":"2024-12-28T04:45:12.809709Z"}},"outputs":[{"execution_count":68,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx'],\n        num_rows: 3668\n    })\n    validation: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx'],\n        num_rows: 408\n    })\n    test: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx'],\n        num_rows: 1725\n    })\n})"},"metadata":{}}],"execution_count":68},{"cell_type":"code","source":"raw_datasets['train']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T04:45:12.811677Z","iopub.execute_input":"2024-12-28T04:45:12.811998Z","iopub.status.idle":"2024-12-28T04:45:12.817632Z","shell.execute_reply.started":"2024-12-28T04:45:12.811973Z","shell.execute_reply":"2024-12-28T04:45:12.816547Z"}},"outputs":[{"execution_count":69,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['sentence1', 'sentence2', 'label', 'idx'],\n    num_rows: 3668\n})"},"metadata":{}}],"execution_count":69},{"cell_type":"code","source":"raw_datasets['train'][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T04:45:12.818723Z","iopub.execute_input":"2024-12-28T04:45:12.818958Z","iopub.status.idle":"2024-12-28T04:45:12.844094Z","shell.execute_reply.started":"2024-12-28T04:45:12.818937Z","shell.execute_reply":"2024-12-28T04:45:12.842723Z"}},"outputs":[{"execution_count":70,"output_type":"execute_result","data":{"text/plain":"{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n 'label': 1,\n 'idx': 0}"},"metadata":{}}],"execution_count":70},{"cell_type":"code","source":"raw_datasets['train'][0]['idx']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T04:45:12.845075Z","iopub.execute_input":"2024-12-28T04:45:12.845447Z","iopub.status.idle":"2024-12-28T04:45:12.866731Z","shell.execute_reply.started":"2024-12-28T04:45:12.845418Z","shell.execute_reply":"2024-12-28T04:45:12.865578Z"}},"outputs":[{"execution_count":71,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":71},{"cell_type":"code","source":"# Define the checkpoint for the tokenizer\ncheckpoint = \"bert-base-uncased\"  # Use the BERT base model with uncased tokenization\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)  # Load the tokenizer associated with the specified checkpoint","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T04:45:12.867741Z","iopub.execute_input":"2024-12-28T04:45:12.868051Z","iopub.status.idle":"2024-12-28T04:45:12.995621Z","shell.execute_reply.started":"2024-12-28T04:45:12.868028Z","shell.execute_reply":"2024-12-28T04:45:12.994323Z"}},"outputs":[],"execution_count":72},{"cell_type":"code","source":"# Define a function to tokenize input examples\ndef tokenize_function(example):\n    # Tokenize the sentence pairs with truncation to fit the model's input size\n    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n\n# Apply the tokenize function to the entire dataset\ntokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n# Use the map method with `batched=True` to tokenize all examples efficiently in batches","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T04:45:12.996600Z","iopub.execute_input":"2024-12-28T04:45:12.996854Z","iopub.status.idle":"2024-12-28T04:45:13.131989Z","shell.execute_reply.started":"2024-12-28T04:45:12.996830Z","shell.execute_reply":"2024-12-28T04:45:13.130845Z"}},"outputs":[{"name":"stderr","text":"Map: 100%|██████████| 1725/1725 [00:00<00:00, 15963.30 examples/s]\n","output_type":"stream"}],"execution_count":73},{"cell_type":"code","source":"tokenized_datasets['train']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T04:45:13.132962Z","iopub.execute_input":"2024-12-28T04:45:13.133191Z","iopub.status.idle":"2024-12-28T04:45:13.138501Z","shell.execute_reply.started":"2024-12-28T04:45:13.133168Z","shell.execute_reply":"2024-12-28T04:45:13.137528Z"}},"outputs":[{"execution_count":74,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n    num_rows: 3668\n})"},"metadata":{}}],"execution_count":74},{"cell_type":"code","source":"# Create a data collator for dynamically padding input data\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n# This ensures input tensors are padded to the longest sequence in a batch, making them ready for model training","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T04:45:13.139382Z","iopub.execute_input":"2024-12-28T04:45:13.139576Z","iopub.status.idle":"2024-12-28T04:45:13.154909Z","shell.execute_reply.started":"2024-12-28T04:45:13.139557Z","shell.execute_reply":"2024-12-28T04:45:13.154137Z"}},"outputs":[],"execution_count":75},{"cell_type":"code","source":"# Remove unnecessary columns from the tokenized dataset\ntokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n# The columns \"sentence1\", \"sentence2\", and \"idx\" are no longer needed after tokenization,\n# so we remove them to keep only the relevant data for model training.\n\n# Rename the \"label\" column to \"labels\"\ntokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n# The model expects the column containing target values to be named \"labels\",\n# so we rename the \"label\" column accordingly.\n\n# Set the dataset format to PyTorch tensors\ntokenized_datasets.set_format(\"torch\")\n# This ensures the dataset outputs are in a format compatible with PyTorch during training.\n\ntokenized_datasets[\"train\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T04:45:13.156321Z","iopub.execute_input":"2024-12-28T04:45:13.156574Z","iopub.status.idle":"2024-12-28T04:45:13.174690Z","shell.execute_reply.started":"2024-12-28T04:45:13.156552Z","shell.execute_reply":"2024-12-28T04:45:13.173377Z"}},"outputs":[{"execution_count":76,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n    num_rows: 3668\n})"},"metadata":{}}],"execution_count":76},{"cell_type":"code","source":"tokenized_datasets[\"train\"][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T04:45:13.175735Z","iopub.execute_input":"2024-12-28T04:45:13.175999Z","iopub.status.idle":"2024-12-28T04:45:13.183580Z","shell.execute_reply.started":"2024-12-28T04:45:13.175975Z","shell.execute_reply":"2024-12-28T04:45:13.182720Z"}},"outputs":[{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"{'labels': tensor(1),\n 'input_ids': tensor([  101,  2572,  3217,  5831,  5496,  2010,  2567,  1010,  3183,  2002,\n          2170,  1000,  1996,  7409,  1000,  1010,  1997,  9969,  4487, 23809,\n          3436,  2010,  3350,  1012,   102,  7727,  2000,  2032,  2004,  2069,\n          1000,  1996,  7409,  1000,  1010,  2572,  3217,  5831,  5496,  2010,\n          2567,  1997,  9969,  4487, 23809,  3436,  2010,  3350,  1012,   102]),\n 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1]),\n 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1])}"},"metadata":{}}],"execution_count":77},{"cell_type":"code","source":"from torch.utils.data import DataLoader  # Import DataLoader to handle batching and shuffling of data\n\n# Create a DataLoader for the training dataset\ntrain_dataloader = DataLoader(\n    tokenized_datasets[\"train\"],  # Use the training split of the tokenized dataset\n    shuffle=True,  # Shuffle the data during training to improve model generalization\n    batch_size=16,  # Process data in batches of 8 samples\n    collate_fn=data_collator  # Use the data collator for dynamic padding of input sequences\n)\n\n# Create a DataLoader for the validation dataset\ntest_dataloader = DataLoader(\n    tokenized_datasets[\"validation\"],  # Use the validation split of the tokenized dataset\n    batch_size=16,  # Process data in batches of 8 samples\n    collate_fn=data_collator  # Use the same data collator for consistency\n)\n\n# The DataLoaders handle the tokenized datasets and ensure the data is prepared for the model\n# in the correct format with batching, padding, and shuffling (for training).","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T04:45:13.184885Z","iopub.execute_input":"2024-12-28T04:45:13.185125Z","iopub.status.idle":"2024-12-28T04:45:13.197335Z","shell.execute_reply.started":"2024-12-28T04:45:13.185102Z","shell.execute_reply":"2024-12-28T04:45:13.196132Z"}},"outputs":[],"execution_count":78},{"cell_type":"code","source":"for batch in train_dataloader:\n    print(batch['labels'].shape)\n    print(batch['input_ids'].shape)\n    print(batch['attention_mask'].shape)\n    print(batch['token_type_ids'].shape)\n    break  # Break after retrieving the first batch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T04:45:13.198128Z","iopub.execute_input":"2024-12-28T04:45:13.198333Z","iopub.status.idle":"2024-12-28T04:45:13.210741Z","shell.execute_reply.started":"2024-12-28T04:45:13.198313Z","shell.execute_reply":"2024-12-28T04:45:13.209840Z"}},"outputs":[{"name":"stdout","text":"torch.Size([16])\ntorch.Size([16, 66])\ntorch.Size([16, 66])\ntorch.Size([16, 66])\n","output_type":"stream"}],"execution_count":79},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification  # Import the model class for sequence classification\n\n# Load a pre-trained BERT model for sequence classification\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    checkpoint,  # Use the same checkpoint as the tokenizer (e.g., \"bert-base-uncased\")\n    num_labels=2  # Specify the number of labels for the classification task (binary classification in this case)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T04:45:13.212105Z","iopub.execute_input":"2024-12-28T04:45:13.212317Z","iopub.status.idle":"2024-12-28T04:45:13.340473Z","shell.execute_reply.started":"2024-12-28T04:45:13.212297Z","shell.execute_reply":"2024-12-28T04:45:13.339372Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":80},{"cell_type":"code","source":"# Pass the batch through the model to get outputs\noutputs = model(**batch)\n# The double-asterisk (`**`) unpacks the batch dictionary, passing its items as keyword arguments to the model.\n# Typical inputs include 'input_ids', 'attention_mask', and 'labels' for training.\n# Print the loss and logits from the model's outputs\noutputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T04:45:13.341597Z","iopub.execute_input":"2024-12-28T04:45:13.341867Z","iopub.status.idle":"2024-12-28T04:45:13.624487Z","shell.execute_reply.started":"2024-12-28T04:45:13.341842Z","shell.execute_reply":"2024-12-28T04:45:13.623334Z"}},"outputs":[{"execution_count":81,"output_type":"execute_result","data":{"text/plain":"SequenceClassifierOutput(loss=tensor(0.4172, grad_fn=<NllLossBackward0>), logits=tensor([[-0.7412,  0.4300],\n        [-0.7542,  0.4315],\n        [-0.7060,  0.4024],\n        [-0.7343,  0.4190],\n        [-0.7322,  0.4252],\n        [-0.7553,  0.4364],\n        [-0.7518,  0.4171],\n        [-0.7043,  0.4212],\n        [-0.7555,  0.4346],\n        [-0.7553,  0.4268],\n        [-0.7575,  0.4353],\n        [-0.7523,  0.4288],\n        [-0.7334,  0.4182],\n        [-0.7591,  0.4291],\n        [-0.7574,  0.4151],\n        [-0.7537,  0.4197]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"},"metadata":{}}],"execution_count":81},{"cell_type":"code","source":"from transformers import AdamW  # Import the AdamW optimizer from the Hugging Face transformers library\n\n# Initialize the AdamW optimizer for training\noptimizer = AdamW(\n    model.parameters(),  # Pass the model's parameters to the optimizer, so it knows which parameters to update\n    lr=5e-5  # Set the learning rate to 5e-5 (a commonly used learning rate for fine-tuning transformers)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T04:45:13.625827Z","iopub.execute_input":"2024-12-28T04:45:13.626085Z","iopub.status.idle":"2024-12-28T04:45:13.641142Z","shell.execute_reply.started":"2024-12-28T04:45:13.626060Z","shell.execute_reply":"2024-12-28T04:45:13.639809Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}],"execution_count":82},{"cell_type":"code","source":"len(train_dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T04:45:13.641956Z","iopub.execute_input":"2024-12-28T04:45:13.642176Z","iopub.status.idle":"2024-12-28T04:45:13.647179Z","shell.execute_reply.started":"2024-12-28T04:45:13.642153Z","shell.execute_reply":"2024-12-28T04:45:13.646440Z"}},"outputs":[{"execution_count":83,"output_type":"execute_result","data":{"text/plain":"230"},"metadata":{}}],"execution_count":83},{"cell_type":"code","source":"from transformers import get_scheduler  # Import the function to get a learning rate scheduler\n\n# Set the number of training epochs\nnum_epochs = 3  # The number of times the model will iterate over the entire training dataset\n\n# Calculate the total number of training steps based on the number of epochs and the number of batches per epoch\nnum_training_steps = num_epochs * len(train_dataloader)\n\n# Initialize the learning rate scheduler to adjust the learning rate during training\nlr_scheduler = get_scheduler(\n    \"linear\",  # Use a linear learning rate scheduler, where the learning rate decreases linearly from the initial value\n    optimizer=optimizer,  # The optimizer to which the scheduler is applied\n    num_warmup_steps=0,  # No warm-up steps, meaning the learning rate starts at the initial value immediately\n    num_training_steps=num_training_steps,  # The total number of training steps\n)\n\n# Print the total number of training steps to verify the calculation\nprint(num_training_steps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T04:45:13.648222Z","iopub.execute_input":"2024-12-28T04:45:13.648585Z","iopub.status.idle":"2024-12-28T04:45:13.676879Z","shell.execute_reply.started":"2024-12-28T04:45:13.648561Z","shell.execute_reply":"2024-12-28T04:45:13.675667Z"}},"outputs":[{"name":"stdout","text":"690\n","output_type":"stream"}],"execution_count":84},{"cell_type":"code","source":"import torch  \ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nmodel.to(device)\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T04:45:13.677693Z","iopub.execute_input":"2024-12-28T04:45:13.678066Z","iopub.status.idle":"2024-12-28T04:45:13.689362Z","shell.execute_reply.started":"2024-12-28T04:45:13.678043Z","shell.execute_reply":"2024-12-28T04:45:13.688331Z"}},"outputs":[{"execution_count":85,"output_type":"execute_result","data":{"text/plain":"device(type='cpu')"},"metadata":{}}],"execution_count":85},{"cell_type":"code","source":"len(train_dataloader.dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T04:45:13.690691Z","iopub.execute_input":"2024-12-28T04:45:13.691051Z","iopub.status.idle":"2024-12-28T04:45:13.697007Z","shell.execute_reply.started":"2024-12-28T04:45:13.691008Z","shell.execute_reply":"2024-12-28T04:45:13.695863Z"}},"outputs":[{"execution_count":86,"output_type":"execute_result","data":{"text/plain":"3668"},"metadata":{}}],"execution_count":86},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score\n\ndef train(train_dataloader, model, optimizer, lr_scheduler, device):\n    model.train()\n    total_loss = 0\n    total_correct = 0  # Variable to track the number of correct predictions\n    total_samples = 0  # Variable to track the total number of samples\n    all_predictions = []\n    all_labels = []\n    \n    for i, batch in enumerate(train_dataloader):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        total_loss += loss.item()\n\n        # Collect predictions and labels\n        logits = outputs.logits\n        predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n        labels = batch[\"labels\"].cpu().numpy()\n        all_predictions.extend(predictions)\n        all_labels.extend(labels)\n\n        # Calculate accuracy for this batch\n        correct = (predictions == labels).sum()\n        total_correct += correct\n        total_samples += len(labels)\n\n        # Backpropagation and optimization\n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        if i%20 == 0:\n            print(f\"Batch {i+1}/{len(train_dataloader)}, Loss: {loss.item():.4f}\")\n\n    # Calculate overall metrics for training\n    avg_loss = total_loss / len(train_dataloader)\n    accuracy = total_correct / total_samples\n    precision = precision_score(all_labels, all_predictions, average=\"weighted\")\n    recall = recall_score(all_labels, all_predictions, average=\"weighted\")\n    f1 = f1_score(all_labels, all_predictions, average=\"weighted\")\n    print(f\"Training - Avg Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n\n\ndef test(test_dataloader, model, device):\n    model.eval()\n    test_loss = 0\n    total_correct = 0  # Variable to track the number of correct predictions\n    total_samples = 0  # Variable to track the total number of samples\n    all_predictions = []\n    all_labels = []\n    num_batches = len(test_dataloader)\n\n    with torch.no_grad():\n        for batch in test_dataloader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(**batch)\n            test_loss += outputs.loss.item()\n\n            # Collect predictions and labels\n            logits = outputs.logits\n            predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n            labels = batch[\"labels\"].cpu().numpy()\n            all_predictions.extend(predictions)\n            all_labels.extend(labels)\n\n            # Calculate accuracy for this batch\n            correct = (predictions == labels).sum()\n            total_correct += correct\n            total_samples += len(labels)\n\n    # Calculate overall metrics for testing\n    avg_loss = test_loss / num_batches\n    accuracy = total_correct / total_samples\n    precision = precision_score(all_labels, all_predictions, average=\"weighted\")\n    recall = recall_score(all_labels, all_predictions, average=\"weighted\")\n    f1 = f1_score(all_labels, all_predictions, average=\"weighted\")\n    print(f\"Testing - Avg Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n\n\n# Main training/testing loop\nepochs = 2\nfor t in range(epochs):\n    print(f\"Epoch {t+1}/{epochs}\")\n    train(train_dataloader, model, optimizer, lr_scheduler, device)\n    test(test_dataloader, model, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T04:45:13.698541Z","iopub.execute_input":"2024-12-28T04:45:13.698780Z","iopub.status.idle":"2024-12-28T04:49:31.247616Z","shell.execute_reply.started":"2024-12-28T04:45:13.698738Z","shell.execute_reply":"2024-12-28T04:49:31.246419Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/2\nBatch 1/230, Loss: 0.4690\nBatch 21/230, Loss: 0.5483\nBatch 41/230, Loss: 0.6943\nBatch 61/230, Loss: 0.5322\nBatch 81/230, Loss: 0.4847\nBatch 101/230, Loss: 0.6771\nBatch 121/230, Loss: 0.4983\nBatch 141/230, Loss: 0.5697\nBatch 161/230, Loss: 0.4624\nBatch 181/230, Loss: 0.4988\nBatch 201/230, Loss: 0.3310\nBatch 221/230, Loss: 0.6791\nTraining - Avg Loss: 0.5443, Accuracy: 0.7345, Precision: 0.7228, Recall: 0.7345, F1 Score: 0.7083\nTesting - Avg Loss: 0.3976, Accuracy: 0.8260, Precision: 0.8226, Recall: 0.8260, F1 Score: 0.8193\nEpoch 2/2\nBatch 1/230, Loss: 0.2865\nBatch 21/230, Loss: 0.2091\nBatch 41/230, Loss: 0.2962\nBatch 61/230, Loss: 0.2174\nBatch 81/230, Loss: 0.2649\nBatch 101/230, Loss: 0.2068\nBatch 121/230, Loss: 0.1195\nBatch 141/230, Loss: 0.2267\nBatch 161/230, Loss: 0.1033\nBatch 181/230, Loss: 0.1102\nBatch 201/230, Loss: 0.1705\nBatch 221/230, Loss: 0.3569\nTraining - Avg Loss: 0.2756, Accuracy: 0.8934, Precision: 0.8925, Recall: 0.8934, F1 Score: 0.8927\nTesting - Avg Loss: 0.3596, Accuracy: 0.8480, Precision: 0.8494, Recall: 0.8480, F1 Score: 0.8487\n","output_type":"stream"}],"execution_count":87},{"cell_type":"code","source":"import os\n\nsave_path = \"/kaggle/working/model\"\nos.makedirs(save_path, exist_ok=True)\n\nmodel.save_pretrained(save_path)\ntokenizer.save_pretrained(save_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T04:49:31.248689Z","iopub.execute_input":"2024-12-28T04:49:31.249000Z","iopub.status.idle":"2024-12-28T04:49:32.398728Z","shell.execute_reply.started":"2024-12-28T04:49:31.248970Z","shell.execute_reply":"2024-12-28T04:49:32.397478Z"}},"outputs":[{"execution_count":88,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/model/tokenizer_config.json',\n '/kaggle/working/model/special_tokens_map.json',\n '/kaggle/working/model/vocab.txt',\n '/kaggle/working/model/added_tokens.json',\n '/kaggle/working/model/tokenizer.json')"},"metadata":{}}],"execution_count":88},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AutoTokenizer\n\n# Load the saved model and tokenizer\nmodel = AutoModelForSequenceClassification.from_pretrained(\"/kaggle/working/model\")\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/working/model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T04:49:32.399513Z","iopub.execute_input":"2024-12-28T04:49:32.399775Z","iopub.status.idle":"2024-12-28T04:49:32.500963Z","shell.execute_reply.started":"2024-12-28T04:49:32.399737Z","shell.execute_reply":"2024-12-28T04:49:32.499570Z"}},"outputs":[],"execution_count":89},{"cell_type":"code","source":"def make_prediction(text1, text2):\n    inputs = tokenizer(text1, text2, truncation=True, padding=True, return_tensors=\"pt\")\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    model.to(device)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    logits = outputs.logits\n    prediction = torch.argmax(logits, dim=-1).item()\n    return prediction\n\n# Example usage Check  2 sentences are equal\ntext1 = \"The quick brown fox jumps over the lazy dog.\"\ntext2 = \"A fast brown fox leaps over a lazy dog.\"\nprediction = make_prediction(text1, text2)\n\nprint(f\"Prediction: {prediction}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T04:49:32.501695Z","iopub.execute_input":"2024-12-28T04:49:32.502261Z","iopub.status.idle":"2024-12-28T04:49:32.571681Z","shell.execute_reply.started":"2024-12-28T04:49:32.502236Z","shell.execute_reply":"2024-12-28T04:49:32.570004Z"}},"outputs":[{"name":"stdout","text":"Prediction: 1\n","output_type":"stream"}],"execution_count":90},{"cell_type":"code","source":"!pip install ipywidgets\n\nfrom huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T04:49:32.572850Z","iopub.execute_input":"2024-12-28T04:49:32.573111Z","iopub.status.idle":"2024-12-28T04:49:36.412472Z","shell.execute_reply.started":"2024-12-28T04:49:32.573085Z","shell.execute_reply":"2024-12-28T04:49:36.411400Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/site-packages (8.1.5)\nRequirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/site-packages (from ipywidgets) (8.30.0)\nRequirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.10/site-packages (from ipywidgets) (0.2.2)\nRequirement already satisfied: jupyterlab-widgets~=3.0.12 in /usr/local/lib/python3.10/site-packages (from ipywidgets) (3.0.13)\nRequirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/site-packages (from ipywidgets) (5.14.3)\nRequirement already satisfied: widgetsnbextension~=4.0.12 in /usr/local/lib/python3.10/site-packages (from ipywidgets) (4.0.13)\nRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.2)\nRequirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.48)\nRequirement already satisfied: decorator in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\nRequirement already satisfied: stack_data in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\nRequirement already satisfied: typing_extensions>=4.6 in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.12.2)\nRequirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\nRequirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\nRequirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\nRequirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\nRequirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.10/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\nRequirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.1.0)\nRequirement already satisfied: pure-eval in /usr/local/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\nRequirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"933cc509efc649699d0024ddb543ffb8"}},"metadata":{}}],"execution_count":91},{"cell_type":"code","source":"from huggingface_hub import upload_file\n\nupload_file(\n    path_or_fileobj=\"/kaggle/working/model/config.json\",  # Path to the file\n    path_in_repo=\"config.json\",  # Path where the file will be stored in the repo\n    repo_id=\"Parit1/dummy\",  # Replace with your repository ID\n)\nupload_file(\n    path_or_fileobj=\"/kaggle/working/model/model.safetensors\",  # Path to the file\n    path_in_repo=\"model.safetensors\",  # Path where the file will be stored in the repo\n    repo_id=\"Parit1/dummy\",  # Replace with your repository ID\n)\nupload_file(\n    path_or_fileobj=\"/kaggle/working/model/special_tokens_map.json\",  # Path to the file\n    path_in_repo=\"special_tokens_map.json\",  # Path where the file will be stored in the repo\n    repo_id=\"Parit1/dummy\",  # Replace with your repository ID\n)\nupload_file(\n    path_or_fileobj=\"/kaggle/working/model/tokenizer.json\",  # Path to the file\n    path_in_repo=\"tokenizer.json\",  # Path where the file will be stored in the repo\n    repo_id=\"Parit1/dummy\",  # Replace with your repository ID\n)\nupload_file(\n    path_or_fileobj=\"/kaggle/working/model/tokenizer_config.json\",  # Path to the file\n    path_in_repo=\"tokenizer_config.json\",  # Path where the file will be stored in the repo\n    repo_id=\"Parit1/dummy\",  # Replace with your repository ID\n)\nupload_file(\n    path_or_fileobj=\"/kaggle/working/model/vocab.txt\",  # Path to the file\n    path_in_repo=\"vocab.txt\",  # Path where the file will be stored in the repo\n    repo_id=\"Parit1/dummy\",  # Replace with your repository ID\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T04:49:36.413626Z","iopub.execute_input":"2024-12-28T04:49:36.413890Z","iopub.status.idle":"2024-12-28T04:49:49.480437Z","shell.execute_reply.started":"2024-12-28T04:49:36.413859Z","shell.execute_reply":"2024-12-28T04:49:49.479645Z"}},"outputs":[{"name":"stderr","text":"No files have been modified since last commit. Skipping to prevent empty commit.\nWARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\nmodel.safetensors: 100%|██████████| 438M/438M [00:10<00:00, 42.0MB/s] \nNo files have been modified since last commit. Skipping to prevent empty commit.\nWARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\nNo files have been modified since last commit. Skipping to prevent empty commit.\nWARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\nNo files have been modified since last commit. Skipping to prevent empty commit.\nWARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\nNo files have been modified since last commit. Skipping to prevent empty commit.\nWARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n","output_type":"stream"},{"execution_count":92,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Parit1/dummy/commit/f0204ee538826d689d54d28ca84bdfa2a6b057b4', commit_message='Upload vocab.txt with huggingface_hub', commit_description='', oid='f0204ee538826d689d54d28ca84bdfa2a6b057b4', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Parit1/dummy', endpoint='https://huggingface.co', repo_type='model', repo_id='Parit1/dummy'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":92},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AutoTokenizer\n\n# Load the saved model and tokenizer\nmodel = AutoModelForSequenceClassification.from_pretrained(\"Parit1/dummy\")\ntokenizer = AutoTokenizer.from_pretrained(\"Parit1/dummy\")\n\ndef make_prediction(text1, text2):\n    inputs = tokenizer(text1, text2, truncation=True, padding=True, return_tensors=\"pt\")\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    model.to(device)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    logits = outputs.logits\n    prediction = torch.argmax(logits, dim=-1).item()\n    return prediction\n\n# Example usage Check  2 sentences are equal\ntext1 = \"The quick brown fox jumps over the lazy dog.\"\ntext2 = \"A fast brown fox leaps over a lazy dog.\"\nprediction = make_prediction(text1, text2)\n\nprint(f\"Prediction: {prediction}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T04:54:52.231032Z","iopub.execute_input":"2024-12-28T04:54:52.231399Z","iopub.status.idle":"2024-12-28T04:54:52.504262Z","shell.execute_reply.started":"2024-12-28T04:54:52.231371Z","shell.execute_reply":"2024-12-28T04:54:52.502964Z"}},"outputs":[{"name":"stdout","text":"Prediction: 1\n","output_type":"stream"}],"execution_count":95}]}